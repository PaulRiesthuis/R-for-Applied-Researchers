[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to R for Data Analysis",
    "section": "",
    "text": "Preface\nThis book is designed to serve as an accessible and practical introduction to R, aimed primarily at researchers and students who are new to the language. It begins with the fundamentals, providing clear guidance on how to get started, and gradually introduces more applied examples that are particularly relevant to psychological research.\nWith the recent rise of large language models such as ChatGPT, many coding tasks have become easier and more efficient. Throughout this book, I offer practical recommendations on how researchers can responsibly and effectively use these tools to support coding, data analysis, and problem-solving, without sacrificing scientific rigor or understanding. This book is also made with help of large language models.\nA central focus of the book is to equip readers with the skills to conduct analyses that are frequently encountered in psychological research. Each chapter provides step-by-step examples, best practices, and insights into common challenges, so that readers can confidently apply R to their own research questions.\n\nI hope this book serves as a helpful guide for anyone looking to develop a solid foundation in R, while also embracing modern tools that enhance productivity and reproducibility in research.\nYou can cite this resource as:\nRiesthuis, P. (2025). Introduction to data analyses in R.\nPaul Riesthuis",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapter1.html",
    "href": "chapter1.html",
    "title": "1  R and Rstudio environment",
    "section": "",
    "text": "1.1 R and Rstudio\nWhen I first started working with R, I remember being confused by the fact that many people recommended downloading both R and RStudio. I didn’t fully understand why RStudio was necessary.\nThe simplest way to think about it is that RStudio is a convenient interface (an IDE) that makes it easier to write, run, and manage your R code. You use R as the underlying language, and RStudio as the environment that helps you work with that language more effectively.\nThe Rstudio looks as follows:\nThe RStudio interface is divided into four main sections that help you write, run, and manage your R code efficiently:\nSummarized:\nA simple way to remember the sections:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R and Rstudio environment</span>"
    ]
  },
  {
    "objectID": "chapter1.html#r-and-rstudio",
    "href": "chapter1.html#r-and-rstudio",
    "title": "1  R and Rstudio environment",
    "section": "",
    "text": "Source\n\n\nThis is where you write and edit your R scripts or Quarto documents.\nThink of it like a “text editor” for your code.\nYou can save your scripts, organize your work, and run lines or sections of code from here.\n\n\nConsole\n\n\nThis is where R actually executes your code.\nYou can type commands directly and see immediate results.\nIt’s perfect for testing small pieces of code or quickly checking something.\n\n\nEnvironment\n\n\nThis section shows all the variables, data frames, and objects currently stored in your R session.\nIt helps you keep track of what data and results you have available.\nSome tabs also show your command history, so you can easily reuse previous commands.\n\n\nOutput\n\n\nThis section displays plots, graphs, and other outputs generated by your code.\nIt also includes tabs for files, packages, help, and viewer windows.\nYou can inspect your results visually and navigate your project files from here.\n\n\n\n\nSource → write code\nConsole → run code\nEnvironment → see your variables\nOutput → see results and plots",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R and Rstudio environment</span>"
    ]
  },
  {
    "objectID": "chapter1.html#sec-coding",
    "href": "chapter1.html#sec-coding",
    "title": "1  R and Rstudio environment",
    "section": "1.2 Coding",
    "text": "1.2 Coding\nSo, in the source section we can actually code variables. The possibilities here are endless. That is, we can code a single number to be assigned to a variable name “y” but also conduct entire structural equation models, multilevel models, etc. Let’s start with some basics and most essential.\n\n1.2.1 Example 1: Variable with single number\n\nIn R we use “&lt;-” instead of “=” to assign variables.\nYou can copy code always by clicking the icon in the upper right of the code chunk.\n\n\nx &lt;- 7 \nx\n\n[1] 7\n\n\nTip: You can run code by selecting a line and pressing Ctrl + Enter (Windows) or Cmd + Enter (Mac).\n\n\n1.2.2 Example 2: Variable with multiple numbers\nThis is also known as a single row vector\n\nrow_vec &lt;- c(1,2,3,4,5)\nrow_vec\n\n[1] 1 2 3 4 5\n\n\n\n\n1.2.3 Example 3: row vs column vectors\nIn R, the vectors we create are row vectors by default. Sometimes, it’s useful to create a column vector (a vector arranged vertically), especially when working with matrices or data frames.\n\ncol_vec &lt;- matrix(c(1, 2, 3, 4, 5), ncol = 1)\ncol_vec\n\n     [,1]\n[1,]    1\n[2,]    2\n[3,]    3\n[4,]    4\n[5,]    5\n\n\nExplanation:\n\nRow vector: A simple vector created with c(); elements are arranged horizontally when printed.\nColumn vector: Created using matrix() with ncol = 1; elements are arranged vertically.\nVectors are the building blocks for matrices and data frames in R.\nTip: In psychological research, a column often represents a variable (e.g., Age, Score) and a row represents a participant.\n\n\n\n1.2.4 Example 4: Dataframes\nA data frame is like a table in R. It can store columns of different types (numeric, character, logical) and is one of the most common ways to organize data for analysis.\n\ndf &lt;- data.frame(\nName = c(\"Alice\", \"Bob\", \"Charlie\"),\nAge = c(25, 30, 22),\nPassed = c(TRUE, FALSE, TRUE)\n)\n\ndf\n\n     Name Age Passed\n1   Alice  25   TRUE\n2     Bob  30  FALSE\n3 Charlie  22   TRUE\n\n\nExplanation:\n\ndata.frame() creates a table-like structures\nColumns can have different types (numbers, text, logical values).\n\n\n\n1.2.5 Example 5: Accessing and Subsetting Data\n\nYou can access columns of a dataframe with $, e.g., df$Age.\n\n\ndf$Age\n\n[1] 25 30 22\n\ndf$Passed\n\n[1]  TRUE FALSE  TRUE\n\n\nEach column is treated as a vector, so you can perform operations on it just like a normal vector.\n\nYou can access rows using row indices, e.g., df[1, ].\n\n\ndf[1, ]\n\n   Name Age Passed\n1 Alice  25   TRUE\n\ndf[2, ]\n\n  Name Age Passed\n2  Bob  30  FALSE\n\n\nThe syntax is df[row, column]. Leaving the column blank selects all columns (or vice versa).\n\nYou can filter rows using logical conditions. For example, to select all participants who passed:\n\n\ndf[df$Passed == TRUE, ]\n\n     Name Age Passed\n1   Alice  25   TRUE\n3 Charlie  22   TRUE\n\n\nSome useful logical operators in R:\n\n== → equal to\n!= → not equal to\n&gt; → greater than\n&lt; → less than\n&gt;= → greater than or equal to\n&lt;= → less than or equal to\n& → AND\n| → OR\n\nExample: Select participants who passed or are older than 25:\n\ndf[df$Passed == TRUE & df$Age &gt; 24, ]\n\n   Name Age Passed\n1 Alice  25   TRUE\n\n\nThere are also packages in R that others have created that also perform these data manipulations but also more advanced such as “dplyr”.\n\nFor example, we can select Age or Passed columns:\n\n\n# install.packages(dplyr) # Run the code before without \"#\" to install the package\nlibrary(dplyr) # This is to call the packages\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\ndf %&gt;% select(Age)\n\n  Age\n1  25\n2  30\n3  22\n\ndf %&gt;% select(Passed)\n\n  Passed\n1   TRUE\n2  FALSE\n3   TRUE\n\n\n\nOr we can filter only participants who passed the test:\n\ndf %&gt;% filter(Passed == TRUE)\n\n     Name Age Passed\n1   Alice  25   TRUE\n2 Charlie  22   TRUE\n\n\nOr select and filter simultaneously for which we need “%&gt;%” (pipe) operator:\n\ndf %&gt;%\nfilter(Passed == TRUE) %&gt;%\nselect(Name, Age)\n\n     Name Age\n1   Alice  25\n2 Charlie  22\n\n\n\nWhy dplyr is useful:\n\nSyntax is more readable than base R, especially for beginners.\nYou can chain multiple operations with %&gt;%, which makes your code look like a step-by-step pipeline.\nWorks well for larger datasets and more complex filtering.\n\nThere are many powerful functions in dplyr, and we’ll explore them gradually throughout the book. However, tools like large language models can be extremely helpful when you want to automate your workflow. You can simply describe—in plain language—what you want to do (e.g., “filter rows where Age is above 30 and select only the Name column”), and the model can generate the corresponding R code for you.\n\n\n1.2.6 Example 6: Descriptive statistics\nIt is also quite easy to compute mean, median, standard deviation or other statistics:\n\nmean(df$Age)\n\n[1] 25.66667\n\nmedian(df$Age)\n\n[1] 25\n\nsd(df$Age)\n\n[1] 4.041452\n\nsummary(df)\n\n     Name                Age          Passed       \n Length:3           Min.   :22.00   Mode :logical  \n Class :character   1st Qu.:23.50   FALSE:1        \n Mode  :character   Median :25.00   TRUE :2        \n                    Mean   :25.67                  \n                    3rd Qu.:27.50                  \n                    Max.   :30.00                  \n\n\nThere are also packages that will automatically summarise your data and provide additional information such as “psych”.\n\n# install.packages(psych) # Run the code before without \"#\" to install the package\n\nlibrary(psych)\ndescribe(df)\n\nWarning in FUN(newX[, i], ...): no non-missing arguments to min; returning Inf\n\n\nWarning in FUN(newX[, i], ...): no non-missing arguments to max; returning -Inf\n\n\n       vars n  mean   sd median trimmed  mad min  max range skew kurtosis   se\nName*     1 3  2.00 1.00      2    2.00 1.48   1    3     2 0.00    -2.33 0.58\nAge       2 3 25.67 4.04     25   25.67 4.45  22   30     8 0.16    -2.33 2.33\nPassed    3 3   NaN   NA     NA     NaN   NA Inf -Inf  -Inf   NA       NA   NA\n\n\n\n\n1.2.7 Summary and conclusion\nIn this introductory chapter, we explored the basic structure and functionality of R and RStudio. You learned how the main interface is organized, how to write and run simple pieces of code, and how to create and manipulate some of the most fundamental data structures in R, such as vectors and data frames. We also introduced essential tools for inspecting and filtering data, both using base R and the more readable dplyr syntax. These skills form the foundation of nearly all data analysis work in R. In the next chapter, we will build on this foundation by working with a real dataset—importing it, exploring it, and preparing it for analysis—so you can see how these tools apply to actual research scenarios.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R and Rstudio environment</span>"
    ]
  },
  {
    "objectID": "chapter2.html",
    "href": "chapter2.html",
    "title": "2  Packages",
    "section": "",
    "text": "2.1 Installing and loading packages\nIn the previous chapter, we explored the RStudio interface, learned how to write and run simple pieces of R code, and practiced creating and manipulating basic data structures such as vectors and data frames. These skills form the foundation for working in R.\nIn this chapter, we take the next step: we will learn how to install packages, load them, and use them in your analyses. Packages are essential in R because they extend the functionality of the language and make data manipulation, visualization, and analysis much easier.\nBy the end of this chapter, you will be able to:\nThe skills you learn here will be applied throughout the rest of the book, as you work with more complex datasets and analyses.\nBase R already includes many useful functions, such as creating variables, working with vectors, and building data frames. However, much of R’s power comes from the thousands of additional packages created by researchers and developers. These packages make tasks like data manipulation, visualization, modeling, or reporting much easier.\nEarlier, we briefly introduced the dplyr package for data manipulation. Let’s now formally look at how to install and load packages.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Packages</span>"
    ]
  },
  {
    "objectID": "chapter2.html#sec-installing-and-loading-packages",
    "href": "chapter2.html#sec-installing-and-loading-packages",
    "title": "2  Packages",
    "section": "",
    "text": "2.1.1 Installing a package from CRAN\nCRAN (the Comprehensive R Archive Network) is the main place where R packages are stored. To install a package from CRAN, you run:\n\ninstall.packages(\"dplyr\")\n\n\nYou only need to install a package once on your computer (unless you update or reinstall R).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Packages</span>"
    ]
  },
  {
    "objectID": "chapter2.html#installing-packages-from-github",
    "href": "chapter2.html#installing-packages-from-github",
    "title": "2  Packages",
    "section": "2.2 Installing packages from GitHub",
    "text": "2.2 Installing packages from GitHub\nNot all packages are available on CRAN. Many developers share their newest or experimental work on GitHub. To install these packages, you first need the devtools package.\n\n2.2.1 Step 1: Install and load devtools\n\ninstall.packages(\"devtools\") \nlibrary(devtools) \n\n\n\n2.2.2 Step 2: Install a package from GitHub\nThe syntax is:\n\ndevtools::install_github(\"username/repository\") \n\nFor example, I created a package called ROCpower and uploaded it to GitHub, you could install it like this\n\ndevtools::install_github(\"PaulRiesthuis/ROCpower\")\n\nGitHub installations are useful when:\n\na package is very new\na feature is only available in the development version\nor the author hasn’t uploaded it to CRAN yet",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Packages</span>"
    ]
  },
  {
    "objectID": "chapter2.html#loading-a-package",
    "href": "chapter2.html#loading-a-package",
    "title": "2  Packages",
    "section": "2.3 Loading a package",
    "text": "2.3 Loading a package\nAfter installation, you need to load the package each time you start a new R session:\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\nOnce a package is loaded, all of its functions become available for use. Riesthuis (2024)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Packages</span>"
    ]
  },
  {
    "objectID": "chapter2.html#using-a-package",
    "href": "chapter2.html#using-a-package",
    "title": "2  Packages",
    "section": "2.4 Using a package",
    "text": "2.4 Using a package\nOnce a package is loaded with library(), you can immediately start using its functions. Most packages are organized around a specific purpose. Some of the most used packages are:\n\n2.4.1 Data manipulation and cleaning\n\ndplyr – grammar of data manipulation (filtering, selecting, grouping)\ntidyr – reshaping and organizing data (pivoting, separating, unnesting)\nstringr – working with text data (string detection, replacement, cleaning)\ntidyverse – Loading tidyverse loads dplyr, tidyr, stringr, and other core packages at once.\n\n\n\n2.4.2 Data importing\n\nreadr – fast reading of CSV and text files\nreadxl – import Excel files (e.g., .xls and .xlsx)\nhaven – import SPSS, Stata, and SAS data\nrvest – downloading or scraping data from the web\n\n\n\n2.4.3 Data visualization\n\nggplot2 – the most widely used visualization system in R\npatchwork – combine multiple ggplots into one figure\nggthemes – additional themes and styles for ggplot2\nplotly – make interactive plots based on ggplot2\n\n\n\n2.4.4 Data analysis and statistics\n\nlme4 – mixed-effects models\npsych – descriptive statistics and psychometric tools\nnegligible – equivalence tests\ncar – common regression diagnostics and hypothesis tests\nmetafor – meta-analyses\nbroom – convert model outputs into tidy data frames\n\n\n\n2.4.5 Simulation\n\nfaux – simulate data for typical experimental designs\nsimstudy – flexible framework to simulate data for studies, including complex dependencies\nmirt – simulate item response theory (IRT) data for psychometrics\nMASS – includes functions for statistical methods and simulation for multivariate normal data\n\n\n\n2.4.6 Power analysis\n\nSpower – conduct simulation based power analyses based on G*Power\nROCpower – power analyses for Receiver Operating Characteristic curves\npwr – traditional analytical power calculations for t-tests, ANOVA, correlations, proportions, etc.\n\n\n\n2.4.7 Reporting and reproducibility\n\nknitr – knitting R Markdown and Quarto documents\nrmarkdown / quarto – reproducible reports and documents\njanitor – cleaning column names and simple frequency tables\n\nThese are just a few of the most commonly used packages—R has thousands more, and we will introduce additional packages throughout the book as they become useful.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Packages</span>"
    ]
  },
  {
    "objectID": "chapter2.html#calling-functions-from-a-package",
    "href": "chapter2.html#calling-functions-from-a-package",
    "title": "2  Packages",
    "section": "2.5 Calling functions from a package",
    "text": "2.5 Calling functions from a package\nIf a package is loaded, you can simply call its functions by name:\n\nlibrary(dplyr)\n\ndf &lt;- data.frame(\n  Name  = c(\"Alice\", \"Bob\", \"Charlie\"),\n  Age   = c(25, 30, 22),\n  Passed = c(TRUE, FALSE, TRUE)\n)\n\ndf %&gt;% \n  filter(Passed == TRUE)\n\n     Name Age Passed\n1   Alice  25   TRUE\n2 Charlie  22   TRUE\n\n\nIf you want to use a function without loading the entire package, you can call it with :::\n\ndplyr::filter(df, Passed == TRUE)\n\n     Name Age Passed\n1   Alice  25   TRUE\n2 Charlie  22   TRUE",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Packages</span>"
    ]
  },
  {
    "objectID": "chapter2.html#getting-help-for-a-package",
    "href": "chapter2.html#getting-help-for-a-package",
    "title": "2  Packages",
    "section": "2.6 Getting help for a package",
    "text": "2.6 Getting help for a package\nR includes extensive documentation for every package and every function.\n\n2.6.1 Get help for the whole package:\n\nhelp(package = \"dplyr\")\n\n\n\n2.6.2 Help for a specific function:\n\n?filter\n\n\n\n2.6.3 Search for function names or keywords:\n\n??\"mean\"",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Packages</span>"
    ]
  },
  {
    "objectID": "chapter2.html#summary",
    "href": "chapter2.html#summary",
    "title": "2  Packages",
    "section": "2.7 Summary",
    "text": "2.7 Summary\nIn this chapter, we learned how to install and load R packages, explored some of the most commonly used packages for data manipulation, visualization, analysis, and reporting, and learned how to access help and call functions. Packages are a core part of R’s power, and you will continue to encounter and use many more throughout the book. With these tools in hand, you are ready to move on to working with real datasets and applying your skills in practical research analyses.\n\n\n\n\nRiesthuis, Paul. 2024. “Simulation-Based Power Analyses for the Smallest Effect Size of Interest: A Confidence-Interval Approach for Minimum-Effect and Equivalence Testing.” Advances in Methods and Practices in Psychological Science 7 (2): 25152459241240722. https://doi.org/10.1177/25152459241240722.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Packages</span>"
    ]
  },
  {
    "objectID": "chapter3.html",
    "href": "chapter3.html",
    "title": "3  Real dataset",
    "section": "",
    "text": "3.1 Dataset\nIn the previous chapter, we learned how to install, load, and use R packages. These tools are essential for real-world data analysis, and now we will start applying them in practice.\nIn this chapter, we take the next step: working with a real dataset in R. We will learn how to import data, inspect its structure, clean and prepare it, and perform the first simple analyses. Working with real data is an important milestone because it brings together all the skills you’ve learned so far—creating variables, manipulating data, and using packages—in a practical research context.\nBy the end of this chapter, you will be able to:\nThese skills form the foundation for all subsequent chapters. Once you are comfortable importing and working with real datasets, you will be ready to move on to more advanced analyses and data visualizations.\nTo start working with a real dataset, I will use my own data of my own study Registered Report: The effects of incentivized lies on memory. In this study, participants (n = 230) were randomly assigned to one of two conditions:\nParticipants completed an adapted Sequential Dyadic Die-Rolling paradigm, where they rolled a die and reported its outcome. Depending on the condition:\nTwo days later, participants completed two forms of memory assessment:\nWe will use this dataset to demonstrate the workflow for importing, viewing, and preparing data for analysis.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Real dataset</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Riesthuis, Paul. 2024. “Simulation-Based\nPower Analyses for the Smallest\nEffect Size of Interest:\nA Confidence-Interval\nApproach for Minimum-Effect and\nEquivalence Testing.” Advances in\nMethods and Practices in Psychological Science 7 (2):\n25152459241240722. https://doi.org/10.1177/25152459241240722.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "chapter3.html#dataset",
    "href": "chapter3.html#dataset",
    "title": "3  Real dataset",
    "section": "",
    "text": "strong-incentive to cheat\nweak-incentive to cheat\n\n\n\nIn the strong-incentive condition, lying helped participants avoid a financial penalty.\nIn the weak-incentive condition, lying produced a prosocial outcome (a benefit for an unknown other).\n\n\n\nSix questions from the Autobiographical Memory Questionnaire\n\n-   Each rated on a 7-point Likert scale\n\nMemory for the die rolls",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Real dataset</span>"
    ]
  },
  {
    "objectID": "chapter3.html#load-in-data",
    "href": "chapter3.html#load-in-data",
    "title": "3  Real dataset",
    "section": "3.2 Load in data",
    "text": "3.2 Load in data\nTo load the dataset, we first need the readxl package, which allows R to import Excel files:\n\nlibrary(readxl)\n\nSince the data is stored in an Excel .xlsx file, we will use read_xlsx() to import it:\n\ndf_original &lt;- read_xlsx(\"Final Data Analysis.xlsx\")\n\nNew names:\n• `Die Roll` -&gt; `Die Roll...51`\n• `Die Roll` -&gt; `Die Roll...52`\n• `Die Roll` -&gt; `Die Roll...53`\n• `Die Roll` -&gt; `Die Roll...54`\n• `Die Roll` -&gt; `Die Roll...55`\n• `Die Roll` -&gt; `Die Roll...56`\n• `Memory Errors` -&gt; `Memory Errors...57`\n• `Die roll color errors` -&gt; `Die roll color errors...58`\n• `Die Roll` -&gt; `Die Roll...61`\n• `Die Roll` -&gt; `Die Roll...62`\n• `Die Roll` -&gt; `Die Roll...63`\n• `Die Roll` -&gt; `Die Roll...64`\n• `Die Roll` -&gt; `Die Roll...65`\n• `Die Roll` -&gt; `Die Roll...66`\n• `Memory Errors` -&gt; `Memory Errors...67`\n• `Die roll color errors` -&gt; `Die roll color errors...68`\n\n\nTip:\nIt’s helpful to keep the imported dataset untouched under a name like df_original. This allows you to return to the clean version easily if mistakes happen during data cleaning or transformation.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Real dataset</span>"
    ]
  },
  {
    "objectID": "chapter3.html#view-the-data",
    "href": "chapter3.html#view-the-data",
    "title": "3  Real dataset",
    "section": "3.3 View the data",
    "text": "3.3 View the data\nThe first step in any data-analysis workflow is to inspect the imported dataset—both to verify that it was loaded correctly and to become familiar with its structure.\nYou can open the dataset in RStudio’s spreadsheet-like viewer with:\n\nView(df_original)\n\nThis allows you to browse the first rows and columns, check variable names, and make sure the data imported successfully.\nYou can also check the first five rows of a dataset\n\nhead(df_original)\n\n# A tibble: 6 × 70\n  Status Progress Duration (in seconds…¹ Finished RecordedDate RecipientLastName\n  &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;                  &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;            \n1 Respo… Progress Duration (in seconds)  Finished Recorded Da… Recipient Last N…\n2 IP Ad… 100      238                    True     44475.27119… &lt;NA&gt;             \n3 IP Ad… 100      397                    True     44475.27834… &lt;NA&gt;             \n4 IP Ad… 100      232                    True     44475.28124… &lt;NA&gt;             \n5 IP Ad… 100      426                    True     44475.29065… &lt;NA&gt;             \n6 IP Ad… 100      403                    True     44475.29385… &lt;NA&gt;             \n# ℹ abbreviated name: ¹​`Duration (in seconds)`\n# ℹ 64 more variables: RecipientFirstName &lt;chr&gt;, RecipientEmail &lt;chr&gt;,\n#   ExternalReference &lt;chr&gt;, DistributionChannel &lt;chr&gt;, UserLanguage &lt;chr&gt;,\n#   Q9_1 &lt;chr&gt;, Q9_2 &lt;chr&gt;, Q9_3 &lt;chr&gt;, Q9_4 &lt;chr&gt;, Q9_5 &lt;chr&gt;, Q9_6 &lt;chr&gt;,\n#   `Thrown Color ` &lt;chr&gt;, `Throw Estimation_1` &lt;chr&gt;,\n#   `Throw Estimation_2` &lt;chr&gt;, `Throw Estimation_3` &lt;chr&gt;,\n#   `Throw Estimation_4` &lt;chr&gt;, `Throw Estimation_5` &lt;chr&gt;, …\n\n\nFrom both functions, you can see that there are many columns of which many are not of interest for the analysis. Also many column names have spaces which complicates variable search in R. Hence, a good first step to clean up the data is to fix these column names. This can be done automatically with the janitor package:\n\nlibrary(janitor)\n\nWarning: package 'janitor' was built under R version 4.3.3\n\ndf_original &lt;- df_original %&gt;% \n  clean_names()\n\nThis is typical in a dataset that has data from for example Qualtrics. Hence, let’s subset the data for the data that we are interested in using dplyr\n\nlibrary(dplyr)\ndf_analysis &lt;- df_original %&gt;%\n  select(condition,\n         subcondition_2,\n         x1_as_i_think_about_the_task_i_can_actually_remember_it,\n         x2_as_i_remember_the_task_i_can_feel_now_the_emotions_that_i_felt_then,\n         x3_overall_i_remember_this_event,\n         reverse_coded_question_4,\n         x5_i_remember_how_i_felt_at_the_time_i_just_recalled,\n         x6_i_remember_what_i_thought_at_the_time_of_the_event_i_just_recalled,\n         total_memory_errors)\n\nTo then make this visually more appealing, we can use the package reactable:\n\nlibrary(reactable)\n\nreactable(\n  df_analysis,\n  searchable = TRUE,\n  pagination = TRUE,\n  defaultPageSize = 5,\n  highlight = TRUE\n)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Real dataset</span>"
    ]
  },
  {
    "objectID": "chapter3.html#viewing-the-data",
    "href": "chapter3.html#viewing-the-data",
    "title": "3  Real dataset",
    "section": "3.3 Viewing the data",
    "text": "3.3 Viewing the data\nThe first step in any data-analysis workflow is to inspect the imported dataset—both to confirm that it loaded correctly and to understand how it is structured.\nYou can open it in RStudio’s spreadsheet viewer using:\n\nView(df_original)\n\nTo preview only the first few rows:\n\nhead(df_original)\n\n# A tibble: 6 × 70\n  Status Progress Duration (in seconds…¹ Finished RecordedDate RecipientLastName\n  &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;                  &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;            \n1 Respo… Progress Duration (in seconds)  Finished Recorded Da… Recipient Last N…\n2 IP Ad… 100      238                    True     44475.27119… &lt;NA&gt;             \n3 IP Ad… 100      397                    True     44475.27834… &lt;NA&gt;             \n4 IP Ad… 100      232                    True     44475.28124… &lt;NA&gt;             \n5 IP Ad… 100      426                    True     44475.29065… &lt;NA&gt;             \n6 IP Ad… 100      403                    True     44475.29385… &lt;NA&gt;             \n# ℹ abbreviated name: ¹​`Duration (in seconds)`\n# ℹ 64 more variables: RecipientFirstName &lt;chr&gt;, RecipientEmail &lt;chr&gt;,\n#   ExternalReference &lt;chr&gt;, DistributionChannel &lt;chr&gt;, UserLanguage &lt;chr&gt;,\n#   Q9_1 &lt;chr&gt;, Q9_2 &lt;chr&gt;, Q9_3 &lt;chr&gt;, Q9_4 &lt;chr&gt;, Q9_5 &lt;chr&gt;, Q9_6 &lt;chr&gt;,\n#   `Thrown Color ` &lt;chr&gt;, `Throw Estimation_1` &lt;chr&gt;,\n#   `Throw Estimation_2` &lt;chr&gt;, `Throw Estimation_3` &lt;chr&gt;,\n#   `Throw Estimation_4` &lt;chr&gt;, `Throw Estimation_5` &lt;chr&gt;, …\n\n\nAt this stage, you may notice that:\n\nthe dataset contains many columns not relevant for analysis\nmany variable names contain spaces or special characters, which complicates coding\nTo address this, we will clean the column names.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Real dataset</span>"
    ]
  },
  {
    "objectID": "chapter3.html#cleaning-column-names",
    "href": "chapter3.html#cleaning-column-names",
    "title": "3  Real dataset",
    "section": "3.4 Cleaning column names",
    "text": "3.4 Cleaning column names\nThe janitor package provides a simple function, clean_names(), that automatically:\n\nremoves spaces and special characters\nconverts names to snake_case\nensures names are syntactically valid\n\n\nlibrary(janitor)\ndf_original &lt;- df_original %&gt;% \n  clean_names()\n\nThis is especially useful for survey data exported from platforms like Qualtrics, which often produce long and messy column names.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Real dataset</span>"
    ]
  },
  {
    "objectID": "chapter3.html#selecting-relevant-variables",
    "href": "chapter3.html#selecting-relevant-variables",
    "title": "3  Real dataset",
    "section": "3.6 Selecting relevant variables",
    "text": "3.6 Selecting relevant variables\nNext, we select the variables needed for our analysis using dplyr, rename the fourth question:\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\ndf_analysis &lt;- df_analysis %&gt;%\n  select(participants,\n         condition,\n         subcondition_2,\n         x1_as_i_think_about_the_task_i_can_actually_remember_it,\n         x2_as_i_remember_the_task_i_can_feel_now_the_emotions_that_i_felt_then,\n         x3_overall_i_remember_this_event,\n         reverse_coded_question_4,\n         x5_i_remember_how_i_felt_at_the_time_i_just_recalled,\n         x6_i_remember_what_i_thought_at_the_time_of_the_event_i_just_recalled,\n         total_memory_errors)\n\ndf_analysis &lt;- df_analysis %&gt;%\n  rename(\n    x4_reverse_coded_question_4 = reverse_coded_question_4\n  )\n\nhead(df_analysis)\n\n# A tibble: 6 × 10\n  participants condition         subcondition_2 x1_as_i_think_about_the_task_i…¹\n         &lt;dbl&gt; &lt;chr&gt;             &lt;chr&gt;                                     &lt;dbl&gt;\n1            1 Unlikely Cheating Honest                                        1\n2            2 Unlikely Cheating Lie                                           1\n3            3 Likely Cheating   Honest                                        3\n4            4 Unlikely Cheating Honest                                        1\n5            5 Unlikely Cheating Lie                                           1\n6            6 Likely Cheating   Lie                                           1\n# ℹ abbreviated name: ¹​x1_as_i_think_about_the_task_i_can_actually_remember_it\n# ℹ 6 more variables:\n#   x2_as_i_remember_the_task_i_can_feel_now_the_emotions_that_i_felt_then &lt;dbl&gt;,\n#   x3_overall_i_remember_this_event &lt;dbl&gt;, x4_reverse_coded_question_4 &lt;dbl&gt;,\n#   x5_i_remember_how_i_felt_at_the_time_i_just_recalled &lt;dbl&gt;,\n#   x6_i_remember_what_i_thought_at_the_time_of_the_event_i_just_recalled &lt;dbl&gt;,\n#   total_memory_errors &lt;dbl&gt;\n\n\nIn this study, they also created an overall belief score by averaging six individual memory and belief items which we can do with dplyr. We compute the overall belief score as the row-wise mean of the six relevant variables. Missing values are ignored using na.rm = TRUE.\n\n# Create overall belief score\ndf_analysis &lt;- df_analysis %&gt;% \n  mutate(\n    overall_belief = rowMeans(\n      select(\n        .,\n        x1_as_i_think_about_the_task_i_can_actually_remember_it,\n        x2_as_i_remember_the_task_i_can_feel_now_the_emotions_that_i_felt_then,\n        x3_overall_i_remember_this_event,\n        x4_reverse_coded_question_4,\n        x5_i_remember_how_i_felt_at_the_time_i_just_recalled,\n        x6_i_remember_what_i_thought_at_the_time_of_the_event_i_just_recalled\n      ),\n      na.rm = TRUE\n    )\n  )\n\nThen we can put this together in a searchable table using the package reactable:\n\nlibrary(reactable)\n\nreactable(\n  df_analysis,\n  searchable = TRUE,\n  pagination = TRUE,\n  defaultPageSize = 5,\n  highlight = TRUE\n)\n\n\n\n\n\nThis allows readers to scroll, search, and browse the data directly in a clean table format.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Real dataset</span>"
    ]
  },
  {
    "objectID": "chapter3.html#removing-inattentive-participants",
    "href": "chapter3.html#removing-inattentive-participants",
    "title": "3  Real dataset",
    "section": "3.5 Removing inattentive participants",
    "text": "3.5 Removing inattentive participants\nWhen inspecting the dataset, you may have noticed that it contains 243 rows, corresponding to 243 participants. However, the final sample reported in the article consists of 230 participants. The difference is due to the exclusion of participants who did not pass the attention checks.\nThe variable attention indicates how many attention checks each participant failed. Following the procedure described in the article, we exclude participants who failed more than three attention checks.\nTo keep the original dataset intact, we create a new dataset (df_analysis) using dplyr that excludes these participants:\n\n# Create a new dataset while keeping the original data unchanged\ndf_analysis &lt;- df_original %&gt;% \n  dplyr::filter(attention &lt;= 3)\n\n# Check the number of remaining participants\nnrow(df_analysis)\n\n[1] 230\n\n\nAfter this step, we are left with 230 rows (participants), which matches the sample size reported in the article.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Real dataset</span>"
    ]
  },
  {
    "objectID": "chapter3.html#data-visualization",
    "href": "chapter3.html#data-visualization",
    "title": "3  Real dataset",
    "section": "3.7 Data visualization",
    "text": "3.7 Data visualization\nNow that we have a cleaned dataset, we can start exploring it visually. Visualization is an essential step in data analysis because it allows us to understand patterns, spot outliers, and communicate results clearly.\n\n3.7.1 Belief and recollection overall\nFirst, we will visualize the overall belief and recollection scores. These scores combine responses from six memory questions. We use ggplot2 for flexible plotting:\n\nlibrary(ggplot2)\nlibrary(ThemePark)\n# Plot\nplot_belief &lt;- ggplot(df_analysis, aes(x = condition, y = overall_belief, fill = condition)) +\n  geom_jitter(\n    width = 0.15,\n    alpha = 0.5,\n    size = 2,\n    shape = 21,\n    color = \"black\"\n  ) +\n  scale_y_continuous(\n    limits = c(1, 7),\n    breaks = 1:7\n  ) +\n  labs(\n    y = \"Belief and Recollection Score\",\n    x = NULL\n  ) +\n  theme_barbie() +\n  theme(\n    legend.position = \"none\",\n    axis.text = element_text(size = 16),\n    axis.title.y = element_text(size = 18),\n    panel.grid = element_blank()\n  )\n\nplot_belief\n\n\n\n\n\n\n\n\n\ngeom_jitter spreads points horizontally so overlapping points are visible\nThe plot shows individual participant scores as well as the overall pattern by condition\ntheme_babie is from ThemePark where many other cool themes are available\n\nYou can also use ggstatsplot for plots with built-in statistical summaries:\n\nlibrary(ggstatsplot)\n\nggbetweenstats(\n  data  = df_analysis,\n  x     = condition,\n  y     = overall_belief,\n)\n\n\n\n\n\n\n\n\n\n\n3.7.2 Belief and recollection individual\nSometimes, it’s useful to examine each question separately instead of combining them into a single overall score. To do this, we need to reshape the dataset from wide to long format.\nIn wide format, each participant has a single row, with responses to different questions stored in separate columns (e.g., x1 to x6). In long format, each row represents one participant’s response to a single question. As a result, each participant contributes multiple rows — one for each question.\nReshaping data in this way is typical when working with multilevel models (also called hierarchical linear models or linear mixed models), where each observation (row) represents a participant-item combination. Long-format datasets also make it easier to generate faceted plots, apply grouped summaries, or run analyses that consider responses at the item level.\nR provides convenient functions for switching between formats: pivot_longer() converts wide data to long, and pivot_wider() converts long data back to wide. Mastering these functions allows you to easily manipulate your dataset depending on the type of analysis or visualization you want to perform.\nIn our dataset, reshaping to long format will result in six rows per participant — one for each question. Once in this format, we can create plots that show responses to each question across conditions, which provides a more detailed view of the data compared with the overall score.\n\nlibrary(tidyr)\ndf_long &lt;- df_analysis %&gt;%\n  pivot_longer(\n    cols = starts_with(\"x\"),\n    names_to = \"question\",\n    values_to = \"score\"\n  )\n\nhead(df_long,12)\n\n# A tibble: 12 × 7\n   participants condition      subcondition_2 total_memory_errors overall_belief\n          &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;                        &lt;dbl&gt;          &lt;dbl&gt;\n 1            1 Unlikely Chea… Honest                          11           1.17\n 2            1 Unlikely Chea… Honest                          11           1.17\n 3            1 Unlikely Chea… Honest                          11           1.17\n 4            1 Unlikely Chea… Honest                          11           1.17\n 5            1 Unlikely Chea… Honest                          11           1.17\n 6            1 Unlikely Chea… Honest                          11           1.17\n 7            2 Unlikely Chea… Lie                             22           1.33\n 8            2 Unlikely Chea… Lie                             22           1.33\n 9            2 Unlikely Chea… Lie                             22           1.33\n10            2 Unlikely Chea… Lie                             22           1.33\n11            2 Unlikely Chea… Lie                             22           1.33\n12            2 Unlikely Chea… Lie                             22           1.33\n# ℹ 2 more variables: question &lt;chr&gt;, score &lt;dbl&gt;\n\n\nNow, each participant has six rows — one per question — making it easy to create faceted plots:\n\nggplot(df_long, aes(x = condition, y = score, fill = condition)) +\n  geom_jitter(width = 0.15, alpha = 0.5, size = 2, shape = 21, color = \"black\") +\n  facet_wrap(~question, \n             scales = \"free_y\") +\n  labs(y = \"Score\", \n       x = NULL) +\n  theme_barbie(base_size = 14) +\n  theme(legend.position = \"none\", \n        axis.text.x = element_text(angle = 45, \n                                   hjust = 1))\n\nWarning in plot_theme(plot): The `base_size` theme element is not defined in\nthe element hierarchy.\n\n\n\n\n\n\n\n\n\n\nEach panel corresponds to one question\nscales = \"free_y\" allows each question’s responses to use its own y-axis, making patterns easier to see\n\n\n\n3.7.3 Memory errors\nFor completeness, we can visualize the total memory errors:\n\nggbetweenstats(\n  data  = df_analysis,\n  x     = condition,\n  y     = total_memory_errors,\n)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Real dataset</span>"
    ]
  },
  {
    "objectID": "chapter3.html#data-analysis",
    "href": "chapter3.html#data-analysis",
    "title": "3  Real dataset",
    "section": "3.8 Data analysis",
    "text": "3.8 Data analysis\nAfter exploring the data visually, the next step is to summarize and analyze it quantitatively. A common first step in any analysis workflow is to examine descriptive statistics, which help us understand the overall patterns in the data, detect potential issues, and prepare for more advanced analyses.\n\n3.8.1 Descriptive statistics\nIn the following code, we will extract the sample size, mean, and standard deviation per group for all variables of interest in our dataset.\n\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Specify the numeric variables you want to summarize\nnumeric_vars &lt;- c(\n  \"x1_as_i_think_about_the_task_i_can_actually_remember_it\",\n  \"x2_as_i_remember_the_task_i_can_feel_now_the_emotions_that_i_felt_then\",\n  \"x3_overall_i_remember_this_event\",\n  \"x4_reverse_coded_question_4\",\n  \"x5_i_remember_how_i_felt_at_the_time_i_just_recalled\",\n  \"x6_i_remember_what_i_thought_at_the_time_of_the_event_i_just_recalled\",\n  \"total_memory_errors\",\n  \"overall_belief\"\n)\n\n# Summary table\nsummary_table &lt;- df_analysis %&gt;%\n  group_by(condition) %&gt;%\n  summarise(\n    N = n(),\n    across(\n      all_of(numeric_vars),\n      list(\n        Mean = ~mean(.x, na.rm = TRUE),\n        SD = ~sd(.x, na.rm = TRUE)\n      ),\n      .names = \"{.col}_{.fn}\"\n    ),\n    .groups = \"drop\"\n  )\n\n# View table\nsummary_table\n\n# A tibble: 2 × 18\n  condition             N x1_as_i_think_about_the_task_…¹ x1_as_i_think_about_…²\n  &lt;chr&gt;             &lt;int&gt;                           &lt;dbl&gt;                  &lt;dbl&gt;\n1 Likely Cheating     114                            1.89                   1.25\n2 Unlikely Cheating   116                            1.84                   1.04\n# ℹ abbreviated names:\n#   ¹​x1_as_i_think_about_the_task_i_can_actually_remember_it_Mean,\n#   ²​x1_as_i_think_about_the_task_i_can_actually_remember_it_SD\n# ℹ 14 more variables:\n#   x2_as_i_remember_the_task_i_can_feel_now_the_emotions_that_i_felt_then_Mean &lt;dbl&gt;,\n#   x2_as_i_remember_the_task_i_can_feel_now_the_emotions_that_i_felt_then_SD &lt;dbl&gt;,\n#   x3_overall_i_remember_this_event_Mean &lt;dbl&gt;, …\n\n\n\nEach row corresponds to a condition\nN is the number of participants per condition.\nColumns ending with _Mean or _SD show the mean and standard deviation for each variable.\nOptional: For an interactive table in an HTML document or Shiny app, you can use reactable:\n\nreactable(summary_table)\n\n\n\n\n\n\n\n\n3.8.2 Internal consistency reliability\nIn the article, they conducted several analyses such as assessing the internal consistency among the AMQ score via Cronbach’s alpha and McDonald’s Omega. We can easily do this in R:\n\nitems &lt;- df_analysis %&gt;%\n  select(\n    x1_as_i_think_about_the_task_i_can_actually_remember_it,\n    x2_as_i_remember_the_task_i_can_feel_now_the_emotions_that_i_felt_then,\n    x3_overall_i_remember_this_event,\n    x4_reverse_coded_question_4,\n    x5_i_remember_how_i_felt_at_the_time_i_just_recalled,\n    x6_i_remember_what_i_thought_at_the_time_of_the_event_i_just_recalled\n  )\nalpha_result &lt;- psych::alpha(items)\nalpha_result$total$raw_alpha\n\n[1] 0.8331445\n\nomega_result &lt;- psych::omega(items, nfactors = 1)\nomega_result$omega_h\n\n[1] 0.8414039",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Real dataset</span>"
    ]
  },
  {
    "objectID": "chapter3.html#inferential-statistics",
    "href": "chapter3.html#inferential-statistics",
    "title": "3  Real dataset",
    "section": "3.9 Inferential statistics",
    "text": "3.9 Inferential statistics\nWe now turn to testing our main hypotheses:\n\nBelief and recollection scores: Participants in the strong-incentive to cheat condition (Likely Cheating) are expected to have lower belief and recollection scores than those in the weak-incentive condition (Unlikely Cheating).\nMemory errors: Participants in the strong-incentive condition are expected to have more memory errors than those in the weak-incentive condition.\n\nWe will examine these hypotheses using both Bayesian and frequentist methods.\n\n3.9.1 Belief and recollection scores\n\n3.9.1.1 Bayesian t-test (directional)\n\nlibrary(BayesFactor)\nbf_belief &lt;- ttestBF(\nx = df_analysis$overall_belief[df_analysis$condition==\"Unlikely Cheating\"],\ny = df_analysis$overall_belief[df_analysis$condition==\"Likely Cheating\"],\nprior = \"medium\",\nnullInterval = c(-Inf, 0) # directional test: strong-incentive &lt; weak-incentive\n)\n\nbf_belief\n\nBayes factor analysis\n--------------\n[1] Alt., r=0.707 -Inf&lt;d&lt;0    : 0.0805335 ±0%\n[2] Alt., r=0.707 !(-Inf&lt;d&lt;0) : 0.3441747 ±0%\n\nAgainst denominator:\n  Null, mu1-mu2 = 0 \n---\nBayes factor type: BFindepSample, JZS\n\n\nInterpretation:\n\nA Bayes factor (BF10) greater than 1 indicates evidence for the alternative hypothesis (strong-incentive &lt; weak-incentive).\n\n-A BF10 less than 1 indicates evidence for the null hypothesis.\n\n\n3.9.1.2 Frequentist t-test\n\n# Frequentist t-test for belief scores\nttest_belief &lt;- t.test(\n  x = df_analysis$overall_belief[df_analysis$condition==\"Unlikely Cheating\"],\n  y = df_analysis$overall_belief[df_analysis$condition==\"Likely Cheating\"],\n  paired = FALSE,\n  var.equal = FALSE,\n  alternative = \"greater\"\n)\nttest_belief\n\n\n    Welch Two Sample t-test\n\ndata:  df_analysis$overall_belief[df_analysis$condition == \"Unlikely Cheating\"] and df_analysis$overall_belief[df_analysis$condition == \"Likely Cheating\"]\nt = 0.90671, df = 227.79, p-value = 0.1828\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n -0.09046978         Inf\nsample estimates:\nmean of x mean of y \n 2.472701  2.362573 \n\n\n\n\n3.9.1.3 Effect size\n\neffectsize::effectsize(ttest_belief)\n\nCohen's d |       95% CI\n------------------------\n0.12      | [-0.10, Inf]\n\n- Estimated using un-pooled SD.\n- One-sided CIs: upper bound fixed at [Inf].\n\n\n\n\n\n3.9.2 Memory errors\n\n3.9.2.1 Bayesian t-test (directional)\n\nlibrary(BayesFactor)\nbf_mem &lt;- ttestBF(\nx = df_analysis$total_memory_errors[df_analysis$condition==\"Unlikely Cheating\"],\ny = df_analysis$total_memory_errors[df_analysis$condition==\"Likely Cheating\"],\nprior = \"medium\",\nnullInterval = c(-Inf, 0) # directional test: strong-incentive &lt; weak-incentive\n)\n\nbf_mem\n\nBayes factor analysis\n--------------\n[1] Alt., r=0.707 -Inf&lt;d&lt;0    : 0.1943251 ±0%\n[2] Alt., r=0.707 !(-Inf&lt;d&lt;0) : 0.1116454 ±0%\n\nAgainst denominator:\n  Null, mu1-mu2 = 0 \n---\nBayes factor type: BFindepSample, JZS\n\n\n\n\n3.9.2.2 Frequentist t-test\n\n# Frequentist t-test for memory scores\nttest_mem &lt;- t.test(\n  x = df_analysis$overall_belief[df_analysis$condition==\"Unlikely Cheating\"],\n  y = df_analysis$overall_belief[df_analysis$condition==\"Likely Cheating\"],\n  paired = FALSE,\n  var.equal = FALSE,\n  alternative = \"greater\"\n)\nttest_mem\n\n\n    Welch Two Sample t-test\n\ndata:  df_analysis$overall_belief[df_analysis$condition == \"Unlikely Cheating\"] and df_analysis$overall_belief[df_analysis$condition == \"Likely Cheating\"]\nt = 0.90671, df = 227.79, p-value = 0.1828\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n -0.09046978         Inf\nsample estimates:\nmean of x mean of y \n 2.472701  2.362573 \n\n\n\n\n3.9.2.3 Effect size\n\neffectsize::effectsize(ttest_mem)\n\nCohen's d |       95% CI\n------------------------\n0.12      | [-0.10, Inf]\n\n- Estimated using un-pooled SD.\n- One-sided CIs: upper bound fixed at [Inf].",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Real dataset</span>"
    ]
  }
]